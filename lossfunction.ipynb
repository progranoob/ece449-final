{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomGrayscale(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform1 = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,64,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128,128, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(128, 128, 1,padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv8 = nn.Conv2d(128, 256, 3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(256, 256, 1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(512, 512, 1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc14 = nn.Linear(512*4*4,1024)\n",
    "        self.drop1 = nn.Dropout2d()\n",
    "        self.fc15 = nn.Linear(1024,1024)\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.fc16 = nn.Linear(1024,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        # print(\" x shape \",x.size())\n",
    "        x = x.view(-1,512*4*4)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc16(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def train_sgd(self,device):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "\n",
    "        path = 'weights.tar'\n",
    "        initepoch = 0\n",
    "\n",
    "        if os.path.exists(path) is not True:\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "#             loss = loss_new()\n",
    "            # optimizer = optim.SGD(self.parameters(),lr=0.01)\n",
    "\n",
    "        else:\n",
    "            checkpoint = torch.load(path)\n",
    "            self.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            initepoch = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in range(initepoch,20):  # loop over the dataset multiple times\n",
    "            timestart = time.time()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device),labels.to(device,dtype= torch.int64)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self(inputs)\n",
    "                l = loss_new(outputs, labels)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += l.item()\n",
    "                # print(\"i \",i)\n",
    "                if i % 500 == 499:  # print every 500 mini-batches\n",
    "                    print('[%d, %5d] loss: %.4f' %\n",
    "                          (epoch, i, running_loss / 500))\n",
    "                    running_loss = 0.0\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    print('Accuracy of the network on the %d tran images: %.3f %%' % (total,\n",
    "                            100.0 * correct / total))\n",
    "                    total = 0\n",
    "                    correct = 0\n",
    "                    torch.save({'epoch':epoch,\n",
    "                                'model_state_dict':net.state_dict(),\n",
    "                                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                                'loss':loss\n",
    "                                },path)\n",
    "\n",
    "            print('epoch %d cost %3f sec' %(epoch,time.time()-timestart))\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "    def test(self,device):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the 10000 test images: %.3f %%' % (\n",
    "                100.0 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_new(outputs, labels):\n",
    "    # sfm=nn.Softmax(1)\n",
    "    # pred=sfm(outputs)\n",
    "    # pred = pred.detach()#detach使得pred requires_grad=False,并且不影响outputs\n",
    "\n",
    "    pred = F.softmax(outputs, dim=1)####是否应该detach????????????/\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # Lo=criterion(labels_update,labels)\n",
    "\n",
    "    Le = -torch.mean(torch.sum(F.log_softmax(outputs, dim=1) * pred, dim=1))\n",
    "    # Lc=criterion(labels_update,pred)-criterion(outputs,pred)\n",
    "    print((torch.log(labels) * pred).shape)\n",
    "    Lc = -torch.mean(torch.sum(torch.log(labels) * pred, dim=1)) - Le\n",
    "    loss_total = Lc /class_num\n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "\n",
    "class PENCIL():\n",
    "    def __init__(self, all_labels_tensor, n_samples, n_classes, n_epochs, lrs, alpha, beta, gamma, K=10, save_losses=False, use_KL=True):\n",
    "        '''\n",
    "        all_labels_tensor: torch tensor, 1-D tensor of labels indexed as in the training dataset object\n",
    "        n_samples: int, length of training dataset\n",
    "        n_epochs: list of positive ints, number of epochs of phases in form [n_epochs_i for i in range(3)]\n",
    "        lrs: list of floats, learnings rates for phases in form [lr_i for i in range(3)]\n",
    "        alpha: coefficient for lo loss\n",
    "        beta: coefficient for le loss\n",
    "        gamma: coefficient for label estimate update\n",
    "        K: int, learning rate multiplier for label estimate updates\n",
    "        save_losses: bool, whether to save losses into list of lists of form [[lc,lo,le] for e in *phase 2 epochs*]\n",
    "        use_KL: bool, whether to use KL loss or crossentropy for phase 3\n",
    "        '''\n",
    "        \n",
    "        self.save_losses = save_losses\n",
    "        self.use_KL = use_KL\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lrs = lrs\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.K = K\n",
    "        \n",
    "        self.CELoss = nn.CrossEntropyLoss()\n",
    "        self.KLLoss = nn.KLDivLoss(reduction='mean') #PENCIL official implementation uses mean, not batchmean\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self._init_y_tilde(all_labels_tensor)\n",
    "        self.y_prev = None\n",
    "        self.losses = []\n",
    "        \n",
    "    def _init_y_tilde(self, all_labels_tensor):\n",
    "        '''\n",
    "        all_labels_tensor: torch tensor, 1-D tensor of labels indexed as in the training dataset object\n",
    "        '''\n",
    "        labels_temp = torch.zeros(all_labels_tensor.size(0), self.n_classes).scatter_(1, all_labels_tensor.view(-1, 1).long(), self.K)\n",
    "        self.y_tilde = labels_temp.numpy()\n",
    "        \n",
    "    def set_lr(self, optimizer, epoch):\n",
    "        '''\n",
    "        Call before inner training loop to update lr based on PENCIL phase\n",
    "        '''\n",
    "        lr = -1\n",
    "        if epoch == 0: lr = self.lrs[0] # Phase 1\n",
    "        elif epoch == self.n_epochs[0]: lr = self.lrs[1] # Phase 2\n",
    "        elif epoch == self.n_epochs[0]+self.n_epochs[1]: lr = self.lrs[2] # Phase 3 \n",
    "        elif epoch == self.n_epochs[0]+self.n_epochs[1]+self.n_epochs[2]//3: # Phase 3 first decay\n",
    "            lr = self.lrs[2]/10\n",
    "        elif epoch == self.n_epochs[0]+self.n_epochs[1]+2*self.n_epochs[2]//3: # Phase 3 second decay\n",
    "            lr = self.lrs[2]/100\n",
    "            \n",
    "        if lr!=-1:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    \n",
    "    def get_loss(self, epoch, outputs, labels, indices):\n",
    "        '''\n",
    "        outputs: un-normalized logits \n",
    "        labels: cuda tensor of noisy labels\n",
    "        indices: cpu tensor of indices for current batch\n",
    "        '''\n",
    "        # Calculate loss based on current phase\n",
    "        if epoch < self.n_epochs[0]: #Phase 1\n",
    "            lc = self.CELoss(outputs, labels)\n",
    "        else:\n",
    "            self.y_prev = cp.deepcopy(self.y_tilde[indices,:]) #Get unnormalized label estimates\n",
    "            self.y_prev = torch.tensor(self.y_prev).float()\n",
    "            self.y_prev = self.y_prev.cuda()\n",
    "            self.y_prev.requires_grad = True\n",
    "            # obtain label distributions (y_hat)\n",
    "            y_h = self.softmax(self.y_prev)\n",
    "            if epoch<self.n_epochs[0]+self.n_epochs[1] or self.use_KL: # During phase 1. \n",
    "                lc = self.KLLoss(self.logsoftmax(self.y_prev),self.softmax(outputs))\n",
    "            else: # During phase 2 use CE if self.use_KL=False\n",
    "                lc = self.CELoss(self.softmax(outputs),self.softmax(y_h))\n",
    "            lo = self.CELoss(y_h, labels) # lo is compatibility loss\n",
    "            le = - torch.mean(torch.mul(self.softmax(outputs), self.logsoftmax(outputs))) # le is entropy loss\n",
    "        # Compute total loss\n",
    "        if epoch < self.n_epochs[0]:\n",
    "            loss = lc\n",
    "        elif epoch < self.n_epochs[0]+self.n_epochs[1]:\n",
    "            loss = lc + self.alpha * lo + self.beta * le\n",
    "            if self.save_losses: self.losses.append([lc.item(),lo.item(),le.item()])\n",
    "        else:\n",
    "            loss = lc\n",
    "        return loss\n",
    "    \n",
    "    def update_y_tilde(self, epoch, indices):\n",
    "        '''\n",
    "        Call this after the backward pass over the loss\n",
    "        ''' \n",
    "        # If in phase 2, update y estimate\n",
    "        if epoch >= self.n_epochs[0] and epoch < self.n_epochs[0]+self.n_epochs[1]:\n",
    "            # update y_tilde by back-propagation\n",
    "            self.y_tilde[indices]+=-self.gamma*self.y_prev.grad.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b2c649e6524a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_sgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-64428351d1eb>\u001b[0m in \u001b[0;36mtrain_sgd\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-8947cabab84a>\u001b[0m in \u001b[0;36mloss_new\u001b[1;34m(outputs, labels)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mLe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;31m# Lc=criterion(labels_update,pred)-criterion(outputs,pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[0mLc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mLe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mloss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLc\u001b[0m \u001b[1;33m/\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "net.train_sgd(device)\n",
    "net.test(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_new(outputs, labels):\n",
    "    # sfm=nn.Softmax(1)\n",
    "    # pred=sfm(outputs)\n",
    "    # pred = pred.detach()#detach使得pred requires_grad=False,并且不影响outputs\n",
    "\n",
    "    pred = F.softmax(outputs, dim=1)####是否应该detach????????????/\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # Lo=criterion(labels_update,labels)\n",
    "\n",
    "    Le = -torch.mean(torch.sum(F.log_softmax(outputs, dim=1) * pred, dim=1))\n",
    "    # Lc=criterion(labels_update,pred)-criterion(outputs,pred)\n",
    "    print((torch.log(labels) * pred).shape)\n",
    "    Lc = -torch.mean(torch.sum(torch.log(labels) * pred, dim=1)) - Le\n",
    "    loss_total = Lc /class_num\n",
    "    return loss_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
